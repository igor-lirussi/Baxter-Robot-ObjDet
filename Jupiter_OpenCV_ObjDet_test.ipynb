{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests for Object detection with OpenCV\n",
    "the following notebook contains some test with different model/images for object detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV: 3.4.5\n",
      "IMPORTS OK\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "print(\"OpenCV: \"+ str(cv2.__version__))\n",
    "print(\"IMPORTS OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./img/obj.png\"\n",
    "#show preview image in notebook\n",
    "show_image = False\n",
    "WIDTH_ROBOT = 960\n",
    "HEIGHT_ROBOT = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions image upload: (600, 960)\n",
      "dimensions robot camera: (600, 960)\n",
      "Same dimensions: True\n"
     ]
    }
   ],
   "source": [
    "#Read image\n",
    "img = cv2.imread(image_path,cv2.IMREAD_COLOR )\n",
    "#calculate h and w\n",
    "(h, w) = img.shape[:2]\n",
    "WIDTH = w\n",
    "HEIGHT = h\n",
    "print(\"dimensions image upload: \"+ str((h,w)) )\n",
    "print(\"dimensions robot camera: \"+ str((HEIGHT_ROBOT, WIDTH_ROBOT)) )\n",
    "print(\"Same dimensions: \"+str((h,w)==(HEIGHT, WIDTH)))\n",
    "\n",
    "if show_image:\n",
    "    #cv2.imshow(\"image\",img) #not working in Jupiter notebook\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet SSD Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Loaded\n"
     ]
    }
   ],
   "source": [
    "#Load net\n",
    "PROTO = \"./models/MobileNetSSD_deploy.prototxt\"\n",
    "MODEL = \"./models/MobileNetSSD_deploy.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(PROTO, MODEL)\n",
    "print(\"Net Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_obj = img.copy() #copy img\n",
    "\n",
    "if show_image:\n",
    "    print(\"BEFORE:\")\n",
    "    plt.imshow(img_obj)\n",
    "    plt.show()\n",
    "\n",
    "RESIZED_DIMENSIONS = (300, 300) # Dimensions net was trained on. \n",
    "IMG_NORM_RATIO = 0.007843 # In grayscale a pixel can range between 0 and 255\n",
    " \n",
    "classes =  [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \n",
    "            \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \n",
    "           \"diningtable\",  \"dog\", \"horse\", \"motorbike\", \"person\", \n",
    "           \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# Capture the frame's height and width\n",
    "(h, w) = img_obj.shape[:2]\n",
    "\n",
    "\n",
    "#FORWARD image\n",
    "# Create a blob. A blob is a group of connected pixels in a binary \n",
    "# frame that share some common property (e.g. grayscale value)\n",
    "# Preprocess the frame to prepare it for deep learning classification\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(img_obj, RESIZED_DIMENSIONS), \n",
    "             IMG_NORM_RATIO, RESIZED_DIMENSIONS, 127.5)\n",
    "\n",
    "# Set the input for the neural network\n",
    "net.setInput(blob)\n",
    "# Predict the objects in the image\n",
    "neural_network_output = net.forward()\n",
    "print(\"detected: \" + str(neural_network_output.shape[2]))\n",
    "\n",
    "\n",
    "#draw bounding box\n",
    "# Put the bounding boxes around the detected objects\n",
    "for i in np.arange(0, neural_network_output.shape[2]):\n",
    "    confidence = neural_network_output[0, 0, i, 2]\n",
    "    # Confidence must be at least x%       \n",
    "    if confidence > 0.20:\n",
    "        idx = int(neural_network_output[0, 0, i, 1])\n",
    "\n",
    "        bounding_box = neural_network_output[0, 0, i, 3:7] * np.array(\n",
    "            [w, h, w, h])\n",
    "\n",
    "        (startX, startY, endX, endY) = bounding_box.astype(\"int\")\n",
    "\n",
    "        label = \"{}: {:.2f}%\".format(classes[idx], confidence * 100) \n",
    "\n",
    "        cv2.rectangle(img_obj, (startX, startY), (\n",
    "            endX, endY), (255,0,0), 2)     \n",
    "\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15    \n",
    "\n",
    "        cv2.putText(img_obj, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)\n",
    "\n",
    "if show_image:\n",
    "    print(\"RESULT:\")       \n",
    "    plt.imshow(img_obj)\n",
    "    plt.show()\n",
    "\n",
    "#SAVE\n",
    "cv2.imwrite(\"./img/save.png\",img_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo Obj Detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Loaded\n"
     ]
    }
   ],
   "source": [
    "#Load net\n",
    "modelConfig  = \"./models/yolov3-openimages.cfg\"\n",
    "modelWeigths = \"./models/yolov3-openimages.weights\"\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfig, modelWeigths)\n",
    "print(\"Net Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_yolo = img.copy() #copy img\n",
    "\n",
    "if show_image:\n",
    "    print(\"BEFORE:\")\n",
    "    plt.imshow(img_yolo)\n",
    "    plt.show()\n",
    "\n",
    "with open('./models/open_images.names', 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "print(\"classes: \" + str(len(classes)))\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNet(modelWeigths, modelConfig)\n",
    "\n",
    "# create input blob \n",
    "\n",
    "scale = 0.00392\n",
    "blob = cv2.dnn.blobFromImage(img_yolo, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "# set input blob for the network\n",
    "net.setInput(blob)\n",
    "\n",
    "# function to get the output layer names \n",
    "# in the architecture\n",
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img_yolo, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    #color = COLORS[class_id]\n",
    "    color = (255,0,0)\n",
    "\n",
    "    cv2.rectangle(img_yolo, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img_yolo, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    \n",
    "    \n",
    "# run inference through the network\n",
    "# and gather predictions from output layers\n",
    "outs = net.forward(get_output_layers(net))\n",
    "\n",
    "# initialization\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.9\n",
    "nms_threshold = 0.9\n",
    "\n",
    "# for each detetion from each output layer \n",
    "# get the confidence, class id, bounding box params\n",
    "# and ignore weak detections (confidence < 0.5)\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * WIDTH)\n",
    "            center_y = int(detection[1] * HEIGHT)\n",
    "            w = int(detection[2] * WIDTH)\n",
    "            h = int(detection[3] * HEIGHT)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "\n",
    "# apply non-max suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "# go through the detections remaining\n",
    "# after nms and draw bounding box\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    draw_bounding_box(img_yolo, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "'''\n",
    "#previously ###########\n",
    "blob = cv2.dnn.blobFromImage(img_yolo, 1 / 255.0, (416, 416),\n",
    "        swapRB=True, crop=False)\n",
    "\n",
    "# Set the input for the neural network\n",
    "net.setInput(blob)\n",
    "# Predict the objects in the image\n",
    "neural_network_output = net.forward()\n",
    "print(\"detected: \" + str(neural_network_output))\n",
    "'''\n",
    "#TODO FIND A WAY TO MAKE IT WORK WITH OPENCV 3 OR OPENCV4\n",
    "\n",
    "if show_image:\n",
    "    print(\"RESULT:\")\n",
    "    plt.imshow(img_yolo)\n",
    "    plt.show()\n",
    "\n",
    "#SAVE\n",
    "cv2.imwrite(\"./img/save.png\",img_yolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Loaded\n"
     ]
    }
   ],
   "source": [
    "#Load net\n",
    "modelConfig  = \"./models/yolov3-openimages.cfg\"\n",
    "modelWeigths = \"./models/yolov3-openimages.weights\"\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfig, modelWeigths)\n",
    "print(\"Net Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 500\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n",
      "(600, 960, 3)\n",
      "(600, 960, 3)\n",
      "(1, 3, 416, 416)\n",
      "(416, 416, 3)\n",
      "YOLO v3 took 1.48017 seconds\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(500, 3)\n",
      "[102 220 225]\n",
      "600 960\n",
      "Plumbing fixture\n",
      "Tin can\n"
     ]
    }
   ],
   "source": [
    "img_yolo = img.copy() #copy img\n",
    "\n",
    "if show_image:\n",
    "    print(\"BEFORE:\")\n",
    "    plt.imshow(img_yolo)\n",
    "    plt.show()\n",
    "\n",
    "with open('./models/open_images.names', 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "print(\"classes: \" + str(len(classes)))\n",
    "labels = classes\n",
    "# Setting minimum probability to eliminate weak predictions\n",
    "probability_minimum = 0.1\n",
    "\n",
    "# Setting threshold for non maximum suppression\n",
    "threshold = 0.1\n",
    "\n",
    "#from https://www.kaggle.com/code/valentynsichkar/yolo-v3-with-opencv/notebook\n",
    "\n",
    "network = cv2.dnn.readNetFromDarknet(modelConfig, modelWeigths)\n",
    "\n",
    "# Getting names of all layers\n",
    "layers_names_all = network.getLayerNames()  # list of layers' names\n",
    "\n",
    "\n",
    "# Getting only output layers' names that we need from YOLO algorithm\n",
    "layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]  # list of layers' names\n",
    "\n",
    "# Check point\n",
    "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "# Getting image shape\n",
    "image_input_shape = img_yolo.shape\n",
    "\n",
    "# Check point\n",
    "print(image_input_shape)  # tuple of (h, w, 3)\n",
    "\n",
    "if show_image:\n",
    "    plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "    plt.imshow(cv2.cvtColor(img_yolo, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img_yolo, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "# Check point\n",
    "print(img_yolo.shape)  # (917, 1222, 3)\n",
    "print(blob.shape)  # (1, 3, 416, 416)\n",
    "\n",
    "blob_to_show = blob[0, :, :, :].transpose(1, 2, 0)\n",
    "print(blob_to_show.shape)  # (416, 416, 3)\n",
    "\n",
    "if show_image:\n",
    "    # Showing 'blob_to_show'\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 5.0)\n",
    "    plt.imshow(blob_to_show)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calculating at the same time, needed time for forward pass\n",
    "network.setInput(blob)  # setting blob as input to the network\n",
    "start = time.time()\n",
    "output_from_network = network.forward(layers_names_output)\n",
    "end = time.time()\n",
    "\n",
    "# Showing spent time for forward pass\n",
    "print('YOLO v3 took {:.5f} seconds'.format(end - start))\n",
    "\n",
    "# Check point\n",
    "print(type(output_from_network))  # <class 'list'>\n",
    "print(type(output_from_network[0]))  # <class 'numpy.ndarray'>\n",
    "\n",
    "# Seed the generator - every time we run the code it will generate by the same rules\n",
    "# In this way we can keep specific colour the same for every class\n",
    "np.random.seed(42)\n",
    "# randint(low, high=None, size=None, dtype='l')\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# Check point\n",
    "print(colours.shape)  # (80, 3)\n",
    "print(colours[0])  # [102 220 225]\n",
    "\n",
    "# Preparing lists for detected bounding boxes, obtained confidences and class's number\n",
    "bounding_boxes = []\n",
    "confidences = []\n",
    "class_numbers = []\n",
    "\n",
    "# Getting spacial dimension of input image\n",
    "h, w = image_input_shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "# Check point\n",
    "print(h, w)  # 917 1222\n",
    "\n",
    "\n",
    "for result in output_from_network:\n",
    "    # Going through all detections from current output layer\n",
    "    for detection in result:\n",
    "        # Getting class for current object\n",
    "        scores = detection[5:]\n",
    "        class_current = np.argmax(scores)\n",
    "\n",
    "        # Getting confidence (probability) for current object\n",
    "        confidence_current = scores[class_current]\n",
    "\n",
    "        # Eliminating weak predictions by minimum probability\n",
    "        if confidence_current > probability_minimum:\n",
    "            # Scaling bounding box coordinates to the initial image size\n",
    "            # YOLO data format keeps center of detected box and its width and height\n",
    "            # That is why we can just elementwise multiply them to the width and height of the image\n",
    "            box_current = detection[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "            # From current box with YOLO format getting top left corner coordinates\n",
    "            # that are x_min and y_min\n",
    "            x_center, y_center, box_width, box_height = box_current.astype('int')\n",
    "            x_min = int(x_center - (box_width / 2))\n",
    "            y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "            # Adding results into prepared lists\n",
    "            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "            confidences.append(float(confidence_current))\n",
    "            class_numbers.append(class_current)\n",
    "            \n",
    "# It is needed to make sure the data type of the boxes is 'int'\n",
    "# and the type of the confidences is 'float'\n",
    "# https://github.com/opencv/opencv/issues/12789\n",
    "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "# Check point\n",
    "# Showing labels of the detected objects\n",
    "for i in range(len(class_numbers)):\n",
    "    print(labels[int(class_numbers[i])])\n",
    "\n",
    "# Saving found labels\n",
    "with open('found_labels.txt', 'w') as f:\n",
    "    for i in range(len(class_numbers)):\n",
    "        f.write(labels[int(class_numbers[i])])\n",
    "        \n",
    "\n",
    "# Checking if there is at least one detected object\n",
    "if len(results) > 0:\n",
    "    # Going through indexes of results\n",
    "    for i in results.flatten():\n",
    "        # Getting current bounding box coordinates\n",
    "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "\n",
    "        # Preparing colour for current bounding box\n",
    "        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n",
    "\n",
    "        # Drawing bounding box on the original image\n",
    "        cv2.rectangle(img_yolo, (x_min, y_min), (x_min + box_width, y_min + box_height),\n",
    "                      colour_box_current, 5)\n",
    "\n",
    "        # Preparing text with label and confidence for current bounding box\n",
    "        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])], confidences[i])\n",
    "\n",
    "        # Putting text with label and confidence on the original image\n",
    "        cv2.putText(img_yolo, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, colour_box_current, 2)\n",
    "        \n",
    "if show_image:\n",
    "    print(\"RESULT:\")\n",
    "    plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "    plt.imshow(img_yolo)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
